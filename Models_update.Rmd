---
title: "Models"
author: "Maarten de Vries"
date: "12/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(lme4)
library(glmnet)
```

Create a test and train set:
```{r}
## 80% of the sample size
smp_size <- floor(0.80 * nrow(df))

## set the seed to make partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)

# Get train and test dataset
train <- df[train_ind, ] %>% select(-happy)
test <- df[-train_ind, ] %>% select(-happy)

# Remove observations with missing response variable
train <- train %>% filter(!is.na(train$happy_cont))
test <- test %>% filter(!is.na(test$happy_cont))
```

```{r}
# Look at the missing values across variables
apply(is.na(train), 2, sum)
apply(is.na(test), 2, sum)

# Function that gets the mode
# https://www.tutorialspoint.com/r/r_mean_median_mode.htm
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
# Impute missing values by substituting the column mode
# as.numeric turns 0-1 factors into 1 and 2, and we need to account for this by subtracting 1 from those columns with 0-1 factors

cols01 = c("urban", "healthy", "church_member", "sport_member", "arts_member", "union_member", "political_party_member", "environment_member", "prof_member", "charity_member", "consumer_member", "self_help_member", "women_member", "religious","political_5","political_6", "immigrant", "immigrant_mother", "immigrant_father", "citizen", "live_with_parents", "breadwinner", "male")

cols_multiple_factors = c("town_size", "settlement_type", "political_1", "political_2","political_3","political_4",'relationship',"education","education_mother", "education_father","employment_status","occupation","sector", "income", "religion")

cols_cont = c('age','household_size',"num_kids")

# Assign a value of the column mode to all missing values
for(col in cols01){
  # Train set
  mod = getmode(as.numeric(train[,col][!is.na(train[,col])]))-1 
  train[,col][is.na(train[,col])] <- as.factor(mod)
  # Test set
  mod = getmode(as.numeric(test[,col][!is.na(test[,col])]))-1
  test[,col][is.na(test[,col])] <- as.factor(mod)
}
for(col in cols_multiple_factors){
  # Train set
  mod = getmode(as.numeric(train[,col][!is.na(train[,col])]))
  train[,col][is.na(train[,col])] <- as.factor(mod)
  # Test set
  mod = getmode(as.numeric(test[,col][!is.na(test[,col])]))
  test[,col][is.na(test[,col])] <- as.factor(mod)
}
for(col in cols_cont){
  # Train set
  mod = getmode(as.numeric(train[,col][!is.na(train[,col])]))
  train[,col][is.na(train[,col])] <- mod
  # Test set
  mod = getmode(as.numeric(test[,col][!is.na(test[,col])]))
  test[,col][is.na(test[,col])] <- mod
}
```


Two simple logistic regression models

Happiness by gender:
```{r}
logit1 = glm(happy_cont ~ male, data=train, family='binomial')
summary(logit1)$coef
```
The odds ratio for being happy comparing men to women is estimated to be `r exp(coef(logit1)[2])` with 95% CI `r exp(confint(logit1))`

Happiness by age:
```{r}
logit2 = glm(happy_cont ~ age, data=train, family='binomial')
summary(logit2)$coef
```
The multiplicative change in odds ratio of being happy for a 1-year increase in age is `r exp(coef(logit2)[2])` with 95% CI `r exp(confint(logit2)[2,])`

Multiple logistic regression model
```{r}
logit3 = glm(happy_cont ~ . -country -year, data=train, family='binomial')

# See exponentiated betas in a sorted list from smallest to largest
exp(as.data.frame(summary(logit3)$coef[,1])) %>% arrange(summary(logit3)$coef[, 1])
```

```{r eval=FALSE}
# TAKES A LONG TIME; Calculting confidence intervals of coefficients
logit3_df = cbind(summary(logit3)$coef[,1], confint(logit3))

# Exponentiate the logistic regression betas
logit3_df_exp = as.data.frame(exp(logit3_df))
colnames(logit3_df_exp) <- c("estimate","2.5","97.5")
logit3_df_exp <- rownames_to_column(logit3_df_exp, "variable")

logit3_df_exp <- logit3_df_exp %>% 
  # Mark as significant if the confidence interval does not contain 0
  mutate(signif = (logit3_df_exp$`2.5` < 1 & logit3_df_exp$`97.5` < 1) |  (logit3_df_exp$`2.5` > 1 & logit3_df_exp$`97.5` > 1)) %>%
  # Sort in order of estimated exponentiated coefficient
  arrange(estimate) 

logit3_df_exp %>% filter(signif == TRUE)
```

```{r}
head(logit3_df_exp, 10) %>% mutate(desc = c('Jewish', 'Separated', 'Orthodox', 'Divorced', 'Unemployed', 'Might join boycotts', 'Widowed', 'Other', 'Single', 'Protestant'))

tail(logit3_df_exp, 10) %>% arrange(-estimate) %>% filter(variable != "(Intercept)") %>% mutate(desc = c('Good health', 'High income', 'Middle income', 'Usually vote in local elections', 'Always vote in local elections', 'Church member', 'Might sign petition', 'Immigrant', 'Secondary'))
```

According to our logistic model, the significant variables that correspond to the lowest odds ratio coefficient are:
- Jewish or Orthodox religion, and less so protestant
- Being  divorced, separated, or widowed
- Being unemployed, self-employed or 'Other'
- Being member of self-help group
- Being the breadwiner

The significant variables that correspond to the higheset odds ratio coefficients are:
- Being healthy
- Having moderate / high income
- Occupation: farmer
- Usually voting in local elections
- Living in a district center
- Signing political petitions
- Being a charity member

Stepwise variable selection in GLM doesn't quite make sense when using categorical factors.

LASSO model and comparison to logit.step

Exploring the use of a LASSO model to get rid of uninformative predictors that get shrunk to zero.
```{r eval=FALSE}
# Fit a Lasso model
X = model.matrix(logit3)[,-1]
lasso.cv = cv.glmnet(x = X, y = train$happy_cont, alpha = 1, family = "multinomial", type.multinomial="grouped"
)
plot(lasso.cv)
```

```{r eval=FALSE}
# Look at the trajectory plot of the coefficients
coefs = coef(lasso.cv,s=lasso.cv$lambda)
num.maineffects = ncol(X) 
matplot(log(lasso.cv$lambda),t(coefs[2:(num.maineffects+1),]),type="l")
abline(v=log(lasso.cv$lambda.min),col="gray",lty=2)
legend(x=6,y=28,colnames(X),lty=1:5,col=1:6,cex=0.5,lwd=1.2)
```

```{r eval=FALSE}
coef(lasso.cv,s=lasso.cv$lambda.1se)
```

Based on the LASSO coefficients shrunk to zero, we decide to drop the 'Age' and 'Sector' predictors.

```{r}
logit.lasso = glm(happy_cont ~ . -country -year -age -sector, data=train, family='binomial')
```

```{r}
logit.lasso.1se = glm(happy_cont ~ . -country -year -age -sector -settlement_type -sport_member -arts_member -union_member -environment_member -prof_member -consumer_member -self_help_member -women_member -religious -political_3 -political_4 -immigrant_mother -immigrant_father -num_kids, data=train, family='binomial')
```


Obtain the predictions and accuracies
```{r}
logit3.pred.train <- ifelse(predict(logit3, train, type="response") > 0.5, 1, 0)
logit3.pred.test <- ifelse(predict(logit3, test, type="response") > 0.5, 1, 0)

logit.step.pred.train = ifelse(predict(logit.lasso, train, type="response") > 0.5, 1, 0)
logit.step.pred.test = ifelse(predict(logit.lasso, test, type="response") > 0.5, 1, 0)

logit.1se.pred.train = ifelse(predict(logit.lasso.1se, train, type="response") > 0.5, 1, 0)
logit.1se.pred.test = ifelse(predict(logit.lasso.1se, test, type="response") > 0.5, 1, 0)

# Accuracy
accuracy <- function(predicted, actual){
  acc = sum(predicted==actual)/length(predicted)
  return(acc)
}

# Table with accuracies
tab = data.frame(logit3=
                   c(42, 
                     accuracy(logit3.pred.train, train$happy_cont),
                     accuracy(logit3.pred.test, test$happy_cont)),
                 logit.lasso.min=
                  c(40,
                    accuracy(logit.step.pred.train,train$happy_cont),
                     accuracy(logit.step.pred.test,test$happy_cont)),
                 logit.lasso.1se=
                  c(25,
                    accuracy(logit.1se.pred.train,train$happy_cont),
                     accuracy(logit.1se.pred.test,test$happy_cont)),
                 baseline=
                   c(0,
                     sum(train$happy_cont)/dim(train)[1], 
                     sum(test$happy_cont)/dim(test)[1]))
rownames(tab) <- c("# predictors","train accuracy", "test accuracy")
tab[2:3,] <- round(tab[2:3,], 3)
tab
```

## Logistic Mixed Effects model, by country

We hypothesize that the distribution of happiness levels are correlated for our samples at the country level. Political and economic circumstances of a country `X` would affect happiness of samples from `X` but not those from countries `Y, Z`. Moreover, people living in some countries, perhaps developed ones, are more likely to be happier than people living in other, likely undeveloped, countries. Existence of scenarios like these makes `country` a natural grouping variable. The below histogram yields support to this argument:

```{r, echo = F}
df %>% 
  mutate(happy_full = happy_full) %>%
  ggplot(aes(x = happy_full)) + 
        geom_histogram(color = "darkorange3", alpha = 0.3) +
        facet_wrap(. ~ country, ncol = 10) +
        labs(
          x = "Happiness Level (1-4)", 
          y = "Number of Respondents",
          title = "Distribution of happiness levels (1-4) by country"
          )

mode <- function(codes){
  which.max(tabulate(codes))
}

happy.modes <- df %>% 
  filter(!is.na(happy_full)) %>% 
  group_by(country) %>%
  summarize(n = mode(happy_full)) %>% 
  ungroup()

happy.perc <- df %>% 
  filter(!is.na(happy_full)) %>% 
  group_by(country, happy_full) %>%
  summarize(n = n()) %>% 
  ungroup() %>% 
  spread(happy_full, n) %>%
  mutate(total = `1` + `2` + `3` + `4`) %>%
  mutate(
    `1` = round(`1` / total, 3), 
    `2` = round(`2` / total, 3), 
    `3` = round(`3` / total, 3), 
    `4` = round(`4` / total, 3)
    )
```
From this, there are 2 main takeaways. First, there exist important distributional differences among countries. Although the mode happiness level is 2 for most states, in developing countries such as `r as.character(happy.modes$country[happy.modes$n == 1])` the most popular response was `1 = Not at all happy`. Meanwhile, all developed countries in the dataset have mode happiness levels higher at `2 = Not very happy`. Second, the number of samples vary largely among countries. This can lead to overfitting for states like `CHL, ARG, CYP` (< 1000 samples, with many `NA` values) if `country` were to be included as a categorical variable. These 2 observations make mixed effect models grouped by `country` highly relevant for this problem. 

As a baseline, we can fit a linear mixed effects model with random intercepts including all variables except `country, year`:
```{r}
glmer1 = glmer(
  happy_cont ~ . - country - year -happy_full + (1 | country), 
  data=train, 
  nAGQ=0,
  family='binomial')
```

```{r}
summary(glmer1)
# Print the random effects
ranef(glmer1)
```

Another variable that should be considered for random effects in the mixed effects model is the `year` variable. The dataset contains 4 years 2017 - 2020, which is not enough to seriously track temporal trends over time.  However, the effect of year on happiness is non-negligible, and using it as a categorical variable fits our purposes. Particularly, some years 

However, different countries are affected by different circumstances in each year which would affect the happiness levels of local samples differently. For example, in the years following up to 2020, Latin American countries were dealing with a severe economic and political crisis due to plummeting oil prices whereas United States was experiencing economic boom. These pre-covid years likely affected Latin Americans differently than Americans. 

```{r}
df %>% 
  mutate(happy_full = happy_full) %>%
  group_by(year) %>%
  summarize(mean_happiness = mean()) %>%
  ungroup() %>%
  ggplot(aes(x = year, y = mode_happiness)) + 
        geom_line(color = "firebrick", alpha = 0.3) +
        facet_wrap(. ~ country, ncol = 10) +
        labs(
          x = "Happiness Level (1-4)", 
          y = "Number of Respondents",
          title = "Distribution of happiness levels (1-4) by country"
          )

```
