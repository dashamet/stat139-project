---
title: "Models"
author: "Maarten de Vries"
date: "12/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(lme4)
library(glmnet)
```

## Creating a test and train set:
```{r}
## 80% of the sample size
smp_size <- floor(0.80 * nrow(df))

## set the seed to make partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(df)), size = smp_size)

# Get train and test dataset
train <- df[train_ind, ]
test <- df[-train_ind, ]

# Remove observations with missing response variable
train <- train %>% filter(!is.na(train$happy))
test <- test %>% filter(!is.na(test$happy))
```

```{r}
# Look at the missing values across variables
apply(is.na(train), 2, sum)
apply(is.na(test), 2, sum)

# Function that gets the mode
# https://www.tutorialspoint.com/r/r_mean_median_mode.htm
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
# Impute missing values by substituting the column mode
# as.numeric turns 0-1 factors into 1 and 2, and we need to account for this by subtracting 1 from those columns with 0-1 factors

cols01 = c("urban", "healthy", "church_member", "sport_member", "arts_member", "union_member", "political_party_member", "environment_member", "prof_member", "charity_member", "consumer_member", "self_help_member", "women_member", "religious","political_5","political_6", "immigrant", "immigrant_mother", "immigrant_father", "citizen", "live_with_parents", "breadwinner", "male")

cols_multiple_factors = c("town_size", "settlement_type", "political_1", "political_2","political_3","political_4",'relationship',"education","education_mother", "education_father","employment_status","occupation","sector", "income", "religion")

cols_cont = c('age','household_size',"num_kids")

# Assign a value of the column mode to all missing values
for(col in cols01){
  # Train set
  mod = getmode(as.numeric(train[,col][!is.na(train[,col])]))-1 
  train[,col][is.na(train[,col])] <- as.factor(mod)
  # Test set
  mod = getmode(as.numeric(test[,col][!is.na(test[,col])]))-1
  test[,col][is.na(test[,col])] <- as.factor(mod)
}
for(col in cols_multiple_factors){
  # Train set
  mod = getmode(as.numeric(train[,col][!is.na(train[,col])]))
  train[,col][is.na(train[,col])] <- as.factor(mod)
  # Test set
  mod = getmode(as.numeric(test[,col][!is.na(test[,col])]))
  test[,col][is.na(test[,col])] <- as.factor(mod)
}
for(col in cols_cont){
  # Train set
  mod = getmode(as.numeric(train[,col][!is.na(train[,col])]))
  train[,col][is.na(train[,col])] <- mod
  # Test set
  mod = getmode(as.numeric(test[,col][!is.na(test[,col])]))
  test[,col][is.na(test[,col])] <- mod
}
```


Two simple logistic regression models

Happiness by gender:
```{r}
logit1 = glm(happy ~ male, data=train, family='binomial')
summary(logit1)$coef
```
The odds ratio for being happy comparing men to women is estimated to be `r exp(coef(logit1)[2])` with 95% CI `r exp(confint(logit1))`

Happiness by age:
```{r}
logit2 = glm(happy ~ age, data=train, family='binomial')
summary(logit2)$coef
```
The multiplicative change in odds ratio of being happy for a 1-year increase in age is `r exp(coef(logit2)[2])` with 95% CI `r exp(confint(logit2)[2,])`

Multiple logistic regression model
```{r}
logit3 = glm(happy ~ . -country -year, data=train, family='binomial')

# See exponentiated betas in a sorted list from smallest to largest
exp(as.data.frame(summary(logit3)$coef[,1])) %>% arrange(summary(logit3)$coef[, 1])
```

```{r eval=FALSE}
# TAKES A LONG TIME; Calculting confidence intervals of coefficients
logit3_df = cbind(summary(logit3)$coef[,1], confint(logit3))

# Exponentiate the logistic regression betas
logit3_df_exp = as.data.frame(exp(logit3_df))
colnames(logit3_df_exp) <- c("estimate","2.5","97.5")
logit3_df_exp <- rownames_to_column(logit3_df_exp, "variable")

logit3_df_exp <- logit3_df_exp %>% 
  # Mark as significant if the confidence interval does not contain 0
  mutate(signif = (logit3_df_exp$`2.5` < 1 & logit3_df_exp$`97.5` < 1) |  (logit3_df_exp$`2.5` > 1 & logit3_df_exp$`97.5` > 1)) %>%
  # Sort in order of estimated exponentiated coefficient
  arrange(estimate) 

logit3_df_exp %>% filter(signif == TRUE)
```

```{r}
head(logit3_df_exp, 10) %>% mutate(desc = c('Jewish', 'Separated', 'Orthodox', 'Divorced', 'Unemployed', 'Might join boycotts', 'Widowed', 'Other', 'Single', 'Protestant'))

tail(logit3_df_exp, 10) %>% arrange(-estimate) %>% filter(variable != "(Intercept)") %>% mutate(desc = c('Good health', 'High income', 'Middle income', 'Usually vote in local elections', 'Always vote in local elections', 'Church member', 'Might sign petition', 'Immigrant', 'Secondary'))
```

According to our logistic model, the significant variables that correspond to the lowest odds ratio coefficient are:
- Jewish or Orthodox religion, and less so protestant
- Being  divorced, separated, or widowed
- Being unemployed, self-employed or 'Other'
- Being member of self-help group
- Being the breadwiner

The significant variables that correspond to the higheset odds ratio coefficients are:
- Being healthy
- Having moderate / high income
- Occupation: farmer
- Usually voting in local elections
- Living in a district center
- Signing political petitions
- Being a charity member

Stepwise variable selection in GLM doesn't quite make sense when using categorical factors.

## Variable Selection

```{r cache = T}
# Takes long to run backward variable selection
logit.step = step(logit3, direction='backward', trace=0)
round(summary(logit.step)$coef,5)
```


## Logistic Mixed Effects model, by country

```{r, echo = F}
# Grid of happiness distributions by country
df %>% 
  mutate(happy_full = happy_full) %>%
  ggplot(aes(x = happy_full)) + 
        geom_histogram(color = "darkorange3", alpha = 0.3) +
        facet_wrap(. ~ country, ncol = 10) +
        labs(
          x = "Happiness Level (1-4)", 
          y = "Number of Respondents",
          title = "Distribution of happiness levels (1-4) by country"
          )

mode <- function(codes){
  which.max(tabulate(codes))
}

# Mode response by country
happy.modes <- df %>% 
  filter(!is.na(happy_full)) %>% 
  group_by(country) %>%
  summarize(n = mode(happy_full)) %>% 
  ungroup()

# Percentage of population for each response
happy.perc <- df %>% 
  filter(!is.na(happy_full)) %>% 
  group_by(country, happy_full) %>%
  summarize(n = n()) %>% 
  ungroup() %>% 
  spread(happy_full, n) %>%
  mutate(total = `1` + `2` + `3` + `4`) %>%
  mutate(
    `1` = round(`1` / total, 3), 
    `2` = round(`2` / total, 3), 
    `3` = round(`3` / total, 3), 
    `4` = round(`4` / total, 3)
    )
```

```{r}
# Random intercept model with country as the main effect
glmer1 = glmer(
  happy ~ . -country -year -arts_member -environment_member 
            -prof_member -consumer_member -women_member 
            -political_3 -age -immigrant_mother 
            -immigrant_father -num_kids -sector   
            + (1 | country), 
  data=train, 
  nAGQ=0, # This argument is necessary for convergence
  family='binomial')
```

```{r}
summary(glmer1)

# Random effects
glmer1.rand_ints <- tibble(ranef(glmer1)$country)
country_names <- as.character(sort(unique(train$country)))
glmer1.rand_ints <- glmer1.rand_ints %>%
  mutate(Country = country_names) %>% 
  rename(Intercept = `(Intercept)`) %>%
  select(Country, Intercept)

# Make a multicolumn table for the paper
library(kableExtra)
kable(
  list(state=glmer1.rand_ints[1:12, ], 
       state=glmer1.rand_ints[13:24, ], 
       state=glmer1.rand_ints[25:36, ],
       state=glmer1.rand_ints[37:48, ])
  )

summary(glmer1)$coef[summary(glmer1)$coeff[, 4] < 0.05, ]
```

```{r}
### Investigate the effect of year

# Get happy_full vector with all 4 happiness levels for our training set
happy_full_train <- df[train_ind, ] %>%
  mutate(happy_full = happy_full[train_ind]) %>% 
  filter(!is.na(happy)) %>%
  pull(happy_full)

# Get a table of mean happiness by year
train %>% 
  mutate(happy_full = happy_full_train) %>%
  group_by(year) %>%
  mutate(happy = as.numeric(happy) - 1) %>%
  summarize(mean_happiness = mean(happy)) %>%
  ungroup()
```
